{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05509bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"labeled_data.csv\")\n",
    "\n",
    "\n",
    "# View first few rows\n",
    "print(df.columns.tolist())\n",
    "df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d37ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ngrok and other Python libraries\n",
    "!pip install fastapi uvicorn \"uvicorn[standard]\" streamlit requests langdetect nest_asyncio pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d78c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile main.py\n",
    "# main.py - This is the backend API using FastAPI\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from typing import Dict, Any\n",
    "import requests\n",
    "import re\n",
    "from langdetect import detect, DetectorFactory\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "\n",
    "# This line helps FastAPI run correctly in Colab\n",
    "nest_asyncio.apply()\n",
    "\n",
    "DetectorFactory.seed = 0 # For consistent language detection\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# --- Configuration ---\n",
    "# IMPORTANT: Replace \"YOUR_HUGGINGFACE_API_TOKEN\" with your actual token\n",
    "HF_API_TOKEN = \"YOUR_HUGGINGFACE_API_TOKEN\"\n",
    "\n",
    "class TextInput(BaseModel):\n",
    "    text: str\n",
    "\n",
    "# --- Mock API Call Functions (Simulate external services) ---\n",
    "# In a real app, these would make actual calls to Google Translate, HuggingFace, etc.\n",
    "\n",
    "async def librettranslate_mock_api(text: str, source_lang: str, target_lang: str) -> Dict[str, Any]:\n",
    "    \"\"\"Mocks LibreTranslate for text translation.\"\"\"\n",
    "    print(f\"MOCK: Translating '{text}' from {source_lang} to {target_lang}\")\n",
    "    mock_translations = {\n",
    "        \"hi\": \"You are so smart\",\n",
    "        \"ur\": \"You are so smart\",\n",
    "        \"ar\": \"You are so smart\",\n",
    "        \"zh\": \"You are so smart\",\n",
    "        \"ru\": \"You are so smart\",\n",
    "        \"es\": \"You are so smart\",\n",
    "        \"fr\": \"You are so smart\",\n",
    "        \"de\": \"You are so smart\",\n",
    "    }\n",
    "    translated_text = mock_translations.get(source_lang, f\"MOCK TRANSLATED: {text}\")\n",
    "    return {\"translatedText\": translated_text}\n",
    "\n",
    "\n",
    "async def huggingface_mock_inference_api(model_name: str, text: str, api_token: str) -> Dict[str, Any]:\n",
    "    \"\"\"Mocks HuggingFace models for sarcasm/hate speech detection.\"\"\"\n",
    "    print(f\"MOCK: Calling HuggingFace model '{model_name}' with text: '{text}'\")\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    if \"sarcasm\" in model_name:\n",
    "        is_sarcastic = False\n",
    "        if \"not\" in text_lower and (\"great\" in text_lower or \"amazing\" in text_lower or \"fantastic\" in text_lower):\n",
    "            is_sarcastic = True\n",
    "        elif re.search(r\"^(wow|oh boy|sure|obviously).*(!|\\.)?$\", text_lower):\n",
    "            is_sarcastic = True\n",
    "        elif len(text.split()) > 3 and all(c.isupper() or not c.isalpha() for c in text):\n",
    "            is_sarcastic = True\n",
    "\n",
    "        return [{\"label\": \"sarcasm\", \"score\": 0.95}] if is_sarcastic else [{\"label\": \"not_sarcasm\", \"score\": 0.98}]\n",
    "\n",
    "    elif \"hate\" in model_name or \"offensive\" in model_name:\n",
    "        label = \"neutral\"\n",
    "        score = 0.99\n",
    "        if any(keyword in text_lower for keyword in [\"idiot\", \"stupid\", \"dumb\", \"loser\"]):\n",
    "            label = \"offensive\"\n",
    "            score = 0.75\n",
    "        if any(keyword in text_lower for keyword in [\"hate\", \"kill\", \"racist\", \"sexist\", \"terrorist\"]):\n",
    "            label = \"hate\"\n",
    "            score = 0.90\n",
    "        if \"f***\" in text_lower or \"shit\" in text_lower or \"bitch\" in text_lower:\n",
    "            label = \"offensive\"\n",
    "            score = 0.80\n",
    "        return [{\"label\": label, \"score\": score}]\n",
    "    else:\n",
    "        return [{\"label\": \"unknown\", \"score\": 0.0}]\n",
    "\n",
    "\n",
    "# --- Main Classification Endpoint ---\n",
    "@app.post(\"/analyze_text\")\n",
    "async def analyze_text(text_input: TextInput):\n",
    "    \"\"\"Processes text for language, sarcasm, and hate speech.\"\"\"\n",
    "    original_text = text_input.text\n",
    "    detected_language = \"en\"\n",
    "    translated_text = original_text\n",
    "\n",
    "    try:\n",
    "        detected_language = detect(original_text)\n",
    "    except Exception:\n",
    "        detected_language = \"en\"\n",
    "\n",
    "    if detected_language != \"en\":\n",
    "        translation_result = await librettranslate_mock_api(original_text, detected_language, \"en\")\n",
    "        translated_text = translation_result.get(\"translatedText\", original_text)\n",
    "\n",
    "    if not translated_text.strip():\n",
    "        translated_text = original_text\n",
    "\n",
    "    sarcasm_model_name = \"cardiffnlp/twitter-roberta-base-sarcasm\"\n",
    "    sarcasm_result = await huggingface_mock_inference_api(sarcasm_model_name, translated_text, HF_API_TOKEN)\n",
    "    sarcasm_label = \"No\"\n",
    "    if sarcasm_result and sarcasm_result[0].get(\"label\") == \"sarcasm\" and sarcasm_result[0].get(\"score\", 0) > 0.5:\n",
    "        sarcasm_label = \"Yes\"\n",
    "\n",
    "    hate_speech_model_name = \"Hate-speech-CNERG/dehatebert-mono-english\"\n",
    "    hate_speech_result = await huggingface_mock_inference_api(hate_speech_model_name, translated_text, HF_API_TOKEN)\n",
    "    hate_speech_label = \"neutral\"\n",
    "    toxicity_score = 0.0\n",
    "\n",
    "    if hate_speech_result and hate_speech_result[0].get(\"label\"):\n",
    "        predicted_label = hate_speech_result[0].get(\"label\").lower()\n",
    "        if predicted_label == \"hate\":\n",
    "            hate_speech_label = \"Hate\"\n",
    "        elif predicted_label == \"offensive\":\n",
    "            hate_speech_label = \"Offensive\"\n",
    "        toxicity_score = hate_speech_result[0].get(\"score\", 0.0)\n",
    "\n",
    "    return {\n",
    "        \"language\": detected_language,\n",
    "        \"translated_text\": translated_text,\n",
    "        \"sarcasm\": sarcasm_label,\n",
    "        \"hate_speech\": hate_speech_label,\n",
    "        \"toxicity_score\": round(toxicity_score, 2)\n",
    "    }\n",
    "```python\n",
    "# Run the FastAPI app in the background\n",
    "uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ad1e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile streamlit_app.py\n",
    "# streamlit_app.py - This is the frontend application using Streamlit\n",
    "\n",
    "import streamlit as st\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# This is where your Streamlit app will talk to your FastAPI backend\n",
    "API_URL = \"http://localhost:8000/analyze_text\"\n",
    "\n",
    "st.title(\"Multi-Language Real-Time Sarcasm & Hate Speech Detector\")\n",
    "st.markdown(\"\"\"\n",
    "    Enter text below to detect language, translate if needed, and analyze for sarcasm,\n",
    "    hate speech, and overall toxicity.\n",
    "    *Note: This is a demo using mock API calls for translation and NLP models.*\n",
    "\"\"\")\n",
    "\n",
    "txt = st.text_area(\"Enter text\", height=120, placeholder=\"Paste a message in any language...\")\n",
    "\n",
    "if st.button(\"Analyze\") and txt.strip():\n",
    "    with st.spinner(\"Analyzing text...\"): # Show a loading message\n",
    "        try:\n",
    "            r = requests.post(API_URL, json={\"text\": txt}, timeout=60)\n",
    "\n",
    "            if r.ok:\n",
    "                res = r.json()\n",
    "\n",
    "                st.write(\"---\")\n",
    "                st.subheader(\"Analysis Results:\")\n",
    "\n",
    "                st.info(f\"**Detected Language:** `{res.get('language', 'Unknown')}`\")\n",
    "                if res.get('language') != 'en':\n",
    "                    st.info(f\"**Translated to English:** `{res.get('translated_text', 'N/A')}`\")\n",
    "\n",
    "                badge = \"ðŸŸ¢ Safe\"\n",
    "                if res.get(\"sarcasm\") == \"Yes\":\n",
    "                    badge = \"ðŸŸ¡ Sarcastic\"\n",
    "                if res.get(\"hate_speech\") == \"Offensive\":\n",
    "                    badge = \"ðŸ”´ Offensive\"\n",
    "                if res.get(\"hate_speech\") == \"Hate\":\n",
    "                    badge = \"ðŸš¨ Hate Speech Detected!\"\n",
    "\n",
    "                st.markdown(f\"### Overall Sentiment: {badge}\")\n",
    "                st.markdown(f\"**Sarcasm:** `{res.get('sarcasm', 'N/A')}`\")\n",
    "                st.markdown(f\"**Hate Speech:** `{res.get('hate_speech', 'N/A')}`\")\n",
    "\n",
    "                toxicity_score = res.get('toxicity_score', 0.0)\n",
    "                st.markdown(f\"**Toxicity Score:** `{toxicity_score}`\")\n",
    "                st.progress(toxicity_score if 0 <= toxicity_score <= 1 else 0.0, text=\"Toxicity Level\")\n",
    "\n",
    "                st.write(\"---\")\n",
    "                st.caption(\"Raw JSON Response:\")\n",
    "                st.json(res)\n",
    "            else:\n",
    "                st.error(f\"API Error: {r.status_code} - {r.text}\")\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            st.error(f\"Could not connect to the API server at {API_URL}. \"\n",
    "                     \"Please ensure the backend is running (check the cell above).\")\n",
    "        except requests.exceptions.Timeout:\n",
    "            st.error(\"The request timed out. The backend API might be slow or unresponsive.\")\n",
    "        except Exception as e:\n",
    "            st.error(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1cc71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate ngrok with your token\n",
    "from pyngrok import ngrok\n",
    "NGROK_AUTH_TOKEN = \"30uXoFuxaNIiu1BnwjwpL8AcPbf_3qZEKDArHnFQ8ex8g7gd6\" \n",
    "ngrok.set_auth_token(NGROK_AUTH_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9defc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Streamlit and tunnel it with ngrok\n",
    "from pyngrok import ngrok\n",
    "import threading\n",
    "import time\n",
    "import subprocess\n",
    "\n",
    "def start_streamlit():\n",
    "    # This will run Streamlit in the background\n",
    "    subprocess.run([\"streamlit\", \"run\", \"streamlit_app.py\", \"--server.port\", \"8501\", \"--server.enableCORS\", \"true\", \"--server.enableXsrfProtection\", \"false\"])\n",
    "\n",
    "# Start Streamlit in a separate thread\n",
    "streamlit_thread = threading.Thread(target=start_streamlit)\n",
    "streamlit_thread.daemon = True\n",
    "streamlit_thread.start()\n",
    "\n",
    "# Give Streamlit a moment to start up\n",
    "time.sleep(5)\n",
    "\n",
    "# Create ngrok tunnel\n",
    "public_url = ngrok.connect(8501)\n",
    "print(f\"Your Streamlit app is available at: {public_url}\")\n",
    "\n",
    "# Keep the cell alive until you manually stop it\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopping ngrok tunnel and Streamlit app.\")\n",
    "    ngrok.kill()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
